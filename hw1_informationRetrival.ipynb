{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbLBD4q7MeDRcEUdGXLvd/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RahafSobh/RahafSobh/blob/main/hw1_informationRetrival.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6vLDUGNNt87D"
      },
      "outputs": [],
      "source": [
        "import re, math, json, random, collections, pathlib, string, os, sys, time\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple, Iterable, Set\n",
        "from collections import Counter, defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ARTIFACTS_DIR = pathlib.Path(\"artifacts\")\n",
        "ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    s = s.lower()\n",
        "    s = s.replace(\"’\",\"'\").replace(\"‘\",\"'\").replace(\"—\",\"-\").replace(\"–\",\"-\")\n",
        "    s = re.sub(r\"[^a-z0-9'\\s-]\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def tokenize(s: str) -> List[str]:\n",
        "    s = normalize_text(s)\n",
        "    tokens = re.findall(r\"[a-z0-9]+(?:'[a-z0-9]+)?\", s)\n",
        "    return tokens\n",
        "\n",
        "print(\"Ready. Artifacts:\", ARTIFACTS_DIR.resolve())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Uff2yfJuaXr",
        "outputId": "d750f647-01a4-4e75-d343-b67d136a296d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ready. Artifacts: /content/artifacts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Porter Stemmer (from scratch)\n",
        "vowels = set(\"aeiou\")\n",
        "def is_consonant(word, i):\n",
        "    ch = word[i]\n",
        "    if ch in vowels: return False\n",
        "    if ch == 'y':\n",
        "        return i == 0 or not is_consonant(word, i-1)\n",
        "    return True\n",
        "\n",
        "def measure(word):\n",
        "    prev_c = None\n",
        "    m = 0\n",
        "    for i,ch in enumerate(word):\n",
        "        c = is_consonant(word, i)\n",
        "        if prev_c is None:\n",
        "            prev_c = c\n",
        "        elif prev_c and not c:\n",
        "            m += 1\n",
        "            prev_c = False\n",
        "        else:\n",
        "            prev_c = c\n",
        "    return m\n",
        "\n",
        "def contains_vowel(stem):\n",
        "    return any(not is_consonant(stem, i) for i in range(len(stem)))\n",
        "\n",
        "def ends_with_double_consonant(word):\n",
        "    return len(word) >= 2 and word[-1] == word[-2] and is_consonant(word, len(word)-1)\n",
        "\n",
        "def ends_cvc(word):\n",
        "    if len(word) < 3: return False\n",
        "    c1 = is_consonant(word, -1)\n",
        "    v = not is_consonant(word, -2)\n",
        "    c0 = is_consonant(word, -3)\n",
        "    if c0 and v and c1:\n",
        "        return word[-1] not in \"wxy\"\n",
        "    return False\n",
        "\n",
        "def porter_stem(w: str) -> str:\n",
        "    word = w\n",
        "    if len(word) <= 2: return word\n",
        "\n",
        "    # Step 1a\n",
        "    if word.endswith(\"sses\"):\n",
        "        word = word[:-2]\n",
        "    elif word.endswith(\"ies\"):\n",
        "        word = word[:-2]\n",
        "    elif word.endswith(\"ss\"):\n",
        "        pass\n",
        "    elif word.endswith(\"s\"):\n",
        "        word = word[:-1]\n",
        "\n",
        "    # Step 1b\n",
        "    if word.endswith(\"eed\"):\n",
        "        stem = word[:-3]\n",
        "        if measure(stem) > 0:\n",
        "            word = stem + \"ee\"\n",
        "    else:\n",
        "        did = False\n",
        "        if word.endswith(\"ed\"):\n",
        "            stem = word[:-2]\n",
        "            if contains_vowel(stem):\n",
        "                word = stem; did = True\n",
        "        elif word.endswith(\"ing\"):\n",
        "            stem = word[:-3]\n",
        "            if contains_vowel(stem):\n",
        "                word = stem; did = True\n",
        "        if did:\n",
        "            if word.endswith((\"at\",\"bl\",\"iz\")):\n",
        "                word += \"e\"\n",
        "            elif ends_with_double_consonant(word) and word[-1] not in \"lsz\":\n",
        "                word = word[:-1]\n",
        "            elif measure(word) == 1 and ends_cvc(word):\n",
        "                word += \"e\"\n",
        "\n",
        "    # Step 1c\n",
        "    if word.endswith(\"y\"):\n",
        "        stem = word[:-1]\n",
        "        if contains_vowel(stem):\n",
        "            word = stem + \"i\"\n",
        "\n",
        "    # Step 2\n",
        "    step2 = {\n",
        "        \"ational\":\"ate\", \"tional\":\"tion\", \"enci\":\"ence\", \"anci\":\"ance\", \"izer\":\"ize\",\n",
        "        \"abli\":\"able\", \"alli\":\"al\", \"entli\":\"ent\", \"eli\":\"e\", \"ousli\":\"ous\",\n",
        "        \"ization\":\"ize\", \"ation\":\"ate\", \"ator\":\"ate\", \"alism\":\"al\", \"iveness\":\"ive\",\n",
        "        \"fulness\":\"ful\", \"ousness\":\"ous\", \"aliti\":\"al\", \"iviti\":\"ive\", \"biliti\":\"ble\", \"logi\":\"log\"\n",
        "    }\n",
        "    for suf, rep in step2.items():\n",
        "        if word.endswith(suf) and measure(word[:-len(suf)]) > 0:\n",
        "            word = word[:-len(suf)] + rep\n",
        "            break\n",
        "\n",
        "    # Step 3\n",
        "    step3 = {\n",
        "        \"icate\":\"ic\",\"ative\":\"\", \"alize\":\"al\",\"iciti\":\"ic\",\"ical\":\"ic\",\n",
        "        \"ful\":\"\", \"ness\":\"\"\n",
        "    }\n",
        "    for suf, rep in step3.items():\n",
        "        if word.endswith(suf) and measure(word[:-len(suf)]) > 0:\n",
        "            word = word[:-len(suf)] + rep\n",
        "            break\n",
        "\n",
        "    # Step 4\n",
        "    step4 = (\"al\",\"ance\",\"ence\",\"er\",\"ic\",\"able\",\"ible\",\"ant\",\"ement\",\"ment\",\"ent\",\n",
        "             \"ion\",\"ou\",\"ism\",\"ate\",\"iti\",\"ous\",\"ive\",\"ize\")\n",
        "    for suf in step4:\n",
        "        if word.endswith(suf):\n",
        "            stem = word[:-len(suf)]\n",
        "            if suf == \"ion\":\n",
        "                if measure(stem) > 1 and (stem.endswith(\"s\") or stem.endswith(\"t\")):\n",
        "                    word = stem\n",
        "                    break\n",
        "            else:\n",
        "                if measure(stem) > 1:\n",
        "                    word = stem\n",
        "                    break\n",
        "\n",
        "    # Step 5\n",
        "    if word.endswith(\"e\"):\n",
        "        stem = word[:-1]\n",
        "        m = measure(stem)\n",
        "        if m > 1 or (m == 1 and not ends_cvc(stem)):\n",
        "            word = stem\n",
        "    if measure(word) > 1 and ends_with_double_consonant(word) and word.endswith(\"l\"):\n",
        "        word = word[:-1]\n",
        "\n",
        "    return word\n",
        "\n",
        "# quick demo\n",
        "for w in [\"caresses\",\"ponies\",\"ties\",\"caress\",\"cats\",\"feed\",\"agreed\",\"plastered\",\"bled\",\n",
        "          \"motoring\",\"sing\",\"conflated\",\"troubled\",\"size\",\"hopping\",\"tanned\",\"falling\",\n",
        "          \"hissing\",\"fizzed\",\"fairly\",\"happiness\",\"relational\"]:\n",
        "    print(w, \"->\", porter_stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmREk5m1uiTp",
        "outputId": "28d2bff4-5737-411e-da91-fd9e170c6c93"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "caresses -> caress\n",
            "ponies -> poni\n",
            "ties -> ti\n",
            "caress -> caress\n",
            "cats -> cat\n",
            "feed -> feed\n",
            "agreed -> agreed\n",
            "plastered -> plaster\n",
            "bled -> bled\n",
            "motoring -> motor\n",
            "sing -> sing\n",
            "conflated -> conflat\n",
            "troubled -> troubl\n",
            "size -> size\n",
            "hopping -> hop\n",
            "tanned -> tan\n",
            "falling -> fall\n",
            "hissing -> hiss\n",
            "fizzed -> fizz\n",
            "fairly -> fairli\n",
            "happiness -> happi\n",
            "relational -> relat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Build Inverted Index for a folder of .txt files\n",
        "DOCS_DIR = \"docs\"  #@param {type:\"string\"}\n",
        "USE_PORTER_STEMMER = True  #@param {type:\"boolean\"}\n",
        "USE_STOPWORDS = True  #@param {type:\"boolean\"}\n",
        "\n",
        "DEFAULT_STOPWORDS = set(\"\"\"\n",
        "the of and to a in that is it for on as with was were be by at are from or an this which\n",
        "\"\"\".split())\n",
        "\n",
        "def preprocess_tokens(text: str) -> List[str]:\n",
        "    toks = tokenize(text)\n",
        "    if USE_STOPWORDS:\n",
        "        toks = [t for t in toks if t not in DEFAULT_STOPWORDS]\n",
        "    if USE_PORTER_STEMMER:\n",
        "        toks = [porter_stem(t) for t in toks]\n",
        "    return toks\n",
        "\n",
        "@dataclass\n",
        "class InvertedIndex:\n",
        "    postings: Dict[str, Dict[str, int]]  # term -> doc_id -> tf\n",
        "    df: Dict[str, int]\n",
        "    idf: Dict[str, float]\n",
        "    doc_lengths: Dict[str, float]  # for cosine normalization\n",
        "    vocab: Set[str]\n",
        "\n",
        "def build_inverted_index(root: str) -> InvertedIndex:\n",
        "    postings = defaultdict(lambda: defaultdict(int))\n",
        "    doc_lengths = {}\n",
        "    vocab = set()\n",
        "    doc_files = []\n",
        "    root_path = pathlib.Path(root)\n",
        "    if not root_path.exists():\n",
        "        root_path.mkdir(parents=True, exist_ok=True)\n",
        "        (root_path/\"doc1.txt\").write_text(\"Cats like milk. Cats chase mice.\")\n",
        "        (root_path/\"doc2.txt\").write_text(\"Dogs chase cats and love bones.\")\n",
        "    for p in root_path.glob(\"*.txt\"):\n",
        "        doc_files.append(p)\n",
        "\n",
        "    for p in doc_files:\n",
        "        text = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "        toks = preprocess_tokens(text)\n",
        "        for t in toks:\n",
        "            postings[t][p.name] += 1\n",
        "            vocab.add(t)\n",
        "\n",
        "    N = len(doc_files)\n",
        "    df = {t: len(docs) for t,docs in postings.items()}\n",
        "    idf = {t: math.log((N + 1) / (df_t + 1)) + 1.0 for t,df_t in df.items()}  # smoothed\n",
        "    # compute doc vector lengths\n",
        "    for p in doc_files:\n",
        "        name = p.name\n",
        "        s = 0.0\n",
        "        for t, d in postings.items():\n",
        "            tf = d.get(name, 0)\n",
        "            if tf:\n",
        "                s += (tf * idf[t])**2\n",
        "        doc_lengths[name] = math.sqrt(s) if s>0 else 1.0\n",
        "\n",
        "    idx = InvertedIndex(dict(postings), df, idf, doc_lengths, vocab)\n",
        "    (ARTIFACTS_DIR / \"inverted_index.json\").write_text(json.dumps({\n",
        "        \"postings\": {k: dict(v) for k,v in idx.postings.items()},\n",
        "        \"df\": idx.df, \"idf\": idx.idf, \"doc_lengths\": idx.doc_lengths\n",
        "    }, indent=2))\n",
        "    print(f\"Indexed {N} docs, vocab size={len(vocab)}; saved to artifacts/inverted_index.json\")\n",
        "    return idx\n",
        "\n",
        "index = build_inverted_index(DOCS_DIR)\n",
        "print(\"Sample vocab:\", sorted(list(index.vocab))[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Hgvqq6duwTN",
        "outputId": "221e6a22-2869-40b9-fec9-fa9fa7cbee23"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexed 2 docs, vocab size=8; saved to artifacts/inverted_index.json\n",
            "Sample vocab: ['bone', 'cat', 'chase', 'dog', 'like', 'love', 'mice', 'milk']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Retrieval functions + demo search\n",
        "QUERY_USE_BOOLEAN = False  #@param {type:\"boolean\"}\n",
        "BOOLEAN_MODE = \"AND\"  #@param [\"AND\",\"OR\"]\n",
        "TOP_K = 5  #@param {type:\"integer\"}\n",
        "\n",
        "def preprocess_query(q: str) -> List[str]:\n",
        "    toks = preprocess_tokens(q)\n",
        "    return toks\n",
        "\n",
        "def search_vector_space(idx: InvertedIndex, query: str, k=5) -> List[Tuple[str, float]]:\n",
        "    q_toks = preprocess_query(query)\n",
        "    if not q_toks: return []\n",
        "    q_tf = Counter(q_toks)\n",
        "    q_vec = {}\n",
        "    for t, tf in q_tf.items():\n",
        "        if t in idx.idf:\n",
        "            q_vec[t] = tf * idx.idf[t]\n",
        "    q_norm = math.sqrt(sum(v*v for v in q_vec.values())) or 1.0\n",
        "\n",
        "    scores = defaultdict(float)\n",
        "    for t, q_w in q_vec.items():\n",
        "        if t not in idx.postings: continue\n",
        "        for doc, tf in idx.postings[t].items():\n",
        "            scores[doc] += q_w * (tf * idx.idf[t])\n",
        "\n",
        "    results = []\n",
        "    for doc, num in scores.items():\n",
        "        denom = (idx.doc_lengths.get(doc,1.0) * q_norm) or 1.0\n",
        "        results.append((doc, num/denom))\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return results[:k]\n",
        "\n",
        "def search_boolean(idx: InvertedIndex, query: str, mode=\"AND\") -> List[str]:\n",
        "    q_toks = preprocess_query(query)\n",
        "    if not q_toks: return []\n",
        "    sets = []\n",
        "    for t in q_toks:\n",
        "        docs = set(idx.postings.get(t,{}).keys())\n",
        "        sets.append(docs)\n",
        "    if not sets: return []\n",
        "    if mode == \"AND\":\n",
        "        s = set.intersection(*sets) if sets else set()\n",
        "    else:\n",
        "        s = set.union(*sets) if sets else set()\n",
        "    return sorted(s)\n",
        "\n",
        "query = \"cats chase\"  #@param {type:\"string\"}\n",
        "if QUERY_USE_BOOLEAN:\n",
        "    print(\"Boolean\", BOOLEAN_MODE, \"→\", search_boolean(index, query, BOOLEAN_MODE))\n",
        "else:\n",
        "    print(\"Vector space results:\")\n",
        "    for doc,score in search_vector_space(index, query, TOP_K):\n",
        "        print(f\"{doc:20s}  score={score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_W0_Rgzu5jF",
        "outputId": "76663643-627f-4265-ba84-8e6765d7777b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector space results:\n",
            "doc1.txt              score=0.6418\n",
            "doc2.txt              score=0.5023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Option 4 — Generate Stop-List (Top-50) from 10 random English Wikipedia pages\n",
        "\n",
        "import time\n",
        "from collections import Counter\n",
        "import pathlib\n",
        "import requests\n",
        "import random\n",
        "\n",
        "ARTIFACTS_DIR = pathlib.Path(\"artifacts\"); ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "N_PAGES = 10\n",
        "SLEEP_BETWEEN = 0.6\n",
        "MAX_ATTEMPTS = 50\n",
        "USE_PORTER_IN_STOPLIST = False\n",
        "FALLBACK_TO_LOCAL_DOCS = True   # if Wikipedia blocked, derive top-50 from your local docs\n",
        "\n",
        "UA = \"IR-Student-Stoplist/1.0 (contact: you@example.com)\"\n",
        "session = requests.Session()\n",
        "session.headers.update({\"User-Agent\": UA, \"Accept\": \"application/json\"})\n",
        "\n",
        "def fetch_random_text_rest():\n",
        "\n",
        "    r = session.get(\"https://en.wikipedia.org/api/rest_v1/page/random/summary\", timeout=15)\n",
        "    r.raise_for_status()\n",
        "    j = r.json()\n",
        "\n",
        "    txt = (j.get(\"extract\") or \"\") + \"\\n\" + (j.get(\"description\") or \"\")\n",
        "    return txt.strip()\n",
        "\n",
        "def fetch_random_text_action():\n",
        "\n",
        "    r = session.get(\"https://en.wikipedia.org/w/api.php\", params={\n",
        "        \"action\": \"query\", \"format\": \"json\", \"list\": \"random\", \"rnnamespace\": 0, \"rnlimit\": 1\n",
        "    }, timeout=15)\n",
        "    r.raise_for_status()\n",
        "    pageid = r.json()[\"query\"][\"random\"][0][\"id\"]\n",
        "    r2 = session.get(\"https://en.wikipedia.org/w/api.php\", params={\n",
        "        \"action\": \"query\", \"format\": \"json\", \"prop\": \"extracts\", \"explaintext\": 1, \"pageids\": pageid\n",
        "    }, timeout=15)\n",
        "    r2.raise_for_status()\n",
        "    page = r2.json()[\"query\"][\"pages\"][str(pageid)]\n",
        "    return page.get(\"extract\", \"\").strip()\n",
        "\n",
        "def fetch_random_wikipedia_plaintext():\n",
        "    # Try REST first; if it fails, try Action API\n",
        "    try:\n",
        "        txt = fetch_random_text_rest()\n",
        "        if txt: return txt\n",
        "    except Exception as e:\n",
        "        pass\n",
        "    # fallback to Action API\n",
        "    txt = fetch_random_text_action()\n",
        "    return txt\n",
        "\n",
        "texts = []\n",
        "attempts = 0\n",
        "while len(texts) < N_PAGES and attempts < MAX_ATTEMPTS:\n",
        "    attempts += 1\n",
        "    try:\n",
        "        t = fetch_random_wikipedia_plaintext()\n",
        "        if t and len(t.split()) > 50:\n",
        "            texts.append(t)\n",
        "            print(f\"Fetched page {len(texts)}/{N_PAGES}, words ~{len(t.split())}\")\n",
        "        else:\n",
        "            print(\"Got very short text, retrying…\")\n",
        "    except requests.HTTPError as e:\n",
        "\n",
        "        print(f\"HTTP error ({e.response.status_code}), backing off…\")\n",
        "        time.sleep(1.5 + random.random())\n",
        "    except Exception as e:\n",
        "        print(\"Error fetching page:\", repr(e))\n",
        "    time.sleep(SLEEP_BETWEEN + random.random()*0.4)\n",
        "\n",
        "if len(texts) < N_PAGES:\n",
        "    print(f\"\\nFetched only {len(texts)}/{N_PAGES} pages after {attempts} attempts.\")\n",
        "    if FALLBACK_TO_LOCAL_DOCS:\n",
        "        print(\"Falling back to building a stop-list from local docs/*.txt\")\n",
        "        import glob, io\n",
        "        docs_text = []\n",
        "        for p in glob.glob(\"docs/*.txt\"):\n",
        "            try:\n",
        "                with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                    docs_text.append(f.read())\n",
        "            except:\n",
        "                pass\n",
        "        if not docs_text:\n",
        "            raise RuntimeError(\"No Wikipedia text and no local docs found. Add some files to docs/ or re-run later.\")\n",
        "        texts = docs_text\n",
        "\n",
        "\n",
        "all_tokens = []\n",
        "for t in texts:\n",
        "    toks = tokenize(t)\n",
        "    if USE_PORTER_IN_STOPLIST:\n",
        "        toks = [porter_stem(x) for x in toks]\n",
        "    all_tokens.extend(toks)\n",
        "\n",
        "counts = Counter(all_tokens)\n",
        "top50 = counts.most_common(50)\n",
        "\n",
        "print(\"\\nTop 50 words:\")\n",
        "for w,c in top50:\n",
        "    print(f\"{w:20s} {c}\")\n",
        "\n",
        "stoplist_path = ARTIFACTS_DIR / \"top_50_stoplist.txt\"\n",
        "with open(stoplist_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for w,_ in top50:\n",
        "        f.write(w+\"\\n\")\n",
        "print(\"\\nSaved:\", stoplist_path.resolve())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29ysMjvMvAzj",
        "outputId": "0e2e59c5-91d9-422a-fe3f-820720f03789"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched page 1/10, words ~67\n",
            "Fetched page 2/10, words ~70\n",
            "Fetched page 3/10, words ~52\n",
            "Got very short text, retrying…\n",
            "Got very short text, retrying…\n",
            "Got very short text, retrying…\n",
            "Fetched page 4/10, words ~61\n",
            "Got very short text, retrying…\n",
            "Got very short text, retrying…\n",
            "Fetched page 5/10, words ~118\n",
            "Fetched page 6/10, words ~67\n",
            "Got very short text, retrying…\n",
            "Fetched page 7/10, words ~62\n",
            "Got very short text, retrying…\n",
            "Fetched page 8/10, words ~102\n",
            "Got very short text, retrying…\n",
            "Got very short text, retrying…\n",
            "Got very short text, retrying…\n",
            "Fetched page 9/10, words ~66\n",
            "Fetched page 10/10, words ~83\n",
            "\n",
            "Top 50 words:\n",
            "the                  67\n",
            "and                  25\n",
            "in                   23\n",
            "of                   23\n",
            "is                   15\n",
            "a                    15\n",
            "by                   14\n",
            "as                   9\n",
            "with                 8\n",
            "to                   7\n",
            "video                7\n",
            "an                   6\n",
            "game                 6\n",
            "was                  6\n",
            "who                  5\n",
            "it                   5\n",
            "on                   5\n",
            "school               5\n",
            "for                  5\n",
            "area                 5\n",
            "or                   5\n",
            "american             4\n",
            "during               4\n",
            "education            4\n",
            "bolsonaro            4\n",
            "interton             4\n",
            "s                    3\n",
            "united               3\n",
            "states               3\n",
            "produced             3\n",
            "which                3\n",
            "first                3\n",
            "new                  3\n",
            "high                 3\n",
            "species              3\n",
            "other                3\n",
            "ideology             3\n",
            "political            3\n",
            "jair                 3\n",
            "right                3\n",
            "console              3\n",
            "film                 3\n",
            "david                3\n",
            "operated             2\n",
            "aircraft             2\n",
            "world                2\n",
            "74                   2\n",
            "news                 2\n",
            "website              2\n",
            "that                 2\n",
            "\n",
            "Saved: /content/artifacts/top_50_stoplist.txt\n"
          ]
        }
      ]
    }
  ]
}